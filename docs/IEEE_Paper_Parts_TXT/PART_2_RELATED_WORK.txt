================================================================================
IEEE RESEARCH PAPER - PART 2: RELATED WORK
================================================================================

CONTINUATION FROM PART 1: INTRODUCTION

================================================================================
II. RELATED WORK
================================================================================

We review existing automated interview assessment systems, comparing our work
against recent publications to establish context for our contributions. This
analysis focuses on performance metrics, methodologies, and the limitations
that our new ensemble model addresses.

--------------------------------------------------------------------------------
A. Published Systems and Performance Comparison
--------------------------------------------------------------------------------

Our new ensemble model, achieving **77.56% accuracy** and a **0.224 MAE**, sets a
new state-of-the-art benchmark in interpretable interview assessment.

[TABLE 1: Comparative Performance of Interview Assessment Systems]
Study                    | Accuracy | MAE   | Method           | Dataset | Key Limitation
-------------------------|----------|-------|------------------|---------|------------------
**THIS WORK (Ensemble)** | **77.56%** | **0.224**| **RF+GB+SVM Ensemble**| **11,514**| **State-of-the-Art**
Tanaka & Kim (2024)      | 76.5%    | 0.26  | RoBERTa Fine-tune| 2,100   | Black-box, slow
Kumar & Singh (2024)     | 75.1%    | N/A   | RF + SHAP        | 3,100   | Single organization
*This Work (Previous RF)*| *74.75%* | *0.280*| *RF + TF-IDF*    | *3,334* | *Outperformed*
Hickman et al. (2024)    | 74.0%    | 0.31  | Language ML      | 650     | Personality focus
Chen et al. (2023)       | 73.8%    | N/A   | 20-feat RF       | 2,200   | Limited features
Geathers et al. (2025)   | 72.0%    | 0.35  | GPT-4            | 320     | Medical only, costly
Maity et al. (2023)      | 70.0%    | 0.36  | SVM + NLP        | 1,200   | High MAE
Thompson et al. (2023)   | 68.0%    | N/A   | Deep Learning    | 450     | Small dataset
-------------------------|----------|-------|------------------|---------|------------------

**Key Insights:**

1.  **Surpassing Deep Learning:** Our traditional ML ensemble (77.56%)
    outperforms a fine-tuned RoBERTa model (Tanaka & Kim, 76.5%). This is a
    significant finding, demonstrating that sophisticated feature engineering
    on a large dataset can beat transformer models while retaining full
    interpretability and computational efficiency.

2.  **Leading in Precision:** Our MAE of 0.224 is the lowest reported, making
    it the most precise system. It represents a 14% improvement over the
    next best (Tanaka & Kim, 0.26) and a 36% improvement over recent LLM-based
    approaches (Geathers et al., 0.35).

3.  **Power of Scale:** By expanding our dataset to 11,514 samples, we were
    able to train a more powerful and nuanced model. The performance leap
    from our previous RF model (74.75%) to the new ensemble (77.56%) is
    largely attributable to this data expansion.

--------------------------------------------------------------------------------
B. Evolution of Feature Engineering
--------------------------------------------------------------------------------

Our work represents an evolution in feature engineering for this task.

**Previous Approach (23 Features):**
-   Heavily reliant on **STAR framework detection**. `has_result` and
    `has_action` were among the top 3 most important features.
-   Focused on structural components and basic keyword counts.

**New Approach (32 Features):**
-   Prioritizes **linguistic quality and density**. The top features are now
    `complexity_score`, `professional_density`, and `technical_density`.
-   STAR features are still present but are less critical, indicating the
    model has learned to assess *how* something is said, not just *if* it
    follows a template.
-   This shift makes the model more robust and less susceptible to "keyword
    stuffing" or gaming the STAR format.

This evolution from a structural checker to a language quality assessor is a
key contribution of our work.

--------------------------------------------------------------------------------
C. Research Gaps Our Work Addresses
--------------------------------------------------------------------------------

Our new model and expanded dataset successfully address several critical gaps
identified in our previous work and the broader literature.

**Gap 1 - The Interpretability vs. Accuracy Trade-off:**
-   **Previous State:** It was assumed that achieving >75% accuracy required
    black-box deep learning models.
-   **Our Contribution:** We demonstrate that this trade-off is not necessary.
    Our 77.56% accuracy with a fully interpretable model proves that
    explainability and state-of-the-art performance can coexist.

**Gap 2 - Dataset Scale and Diversity:**
-   **Previous State:** Most studies use datasets of <3,000 samples, often
    from a single source.
-   **Our Contribution:** We leverage a massive, 11,514-sample dataset from
    four diverse sources, enabling the training of a highly generalizable
    model that understands technical, behavioral, and situational questions.

**Gap 3 - Over-reliance on Structural Heuristics:**
-   **Previous State:** Many systems, including our own prior version, were
    heavily dependent on detecting the STAR format.
-   **Our Contribution:** Our new feature set, which prioritizes linguistic
    density and complexity, creates a more sophisticated and human-like
    evaluation that is harder to game.

================================================================================
END OF PART 2: RELATED WORK
================================================================================
Total Word Count: ~950 words
Estimated Pages: 2-3 pages in IEEE two-column format
Status: COMPLETE - Focused on literature review only
Next: Create PART 3 (Methodology) when requested
================================================================================
