================================================================================
IEEE RESEARCH PAPER - PART 3: METHODOLOGY
================================================================================

CONTINUATION FROM PART 2: RELATED WORK

================================================================================
III. METHODOLOGY
================================================================================

This section details our system architecture, dataset construction, the expanded
feature engineering framework, the NLP preprocessing pipeline, and the
development of our final ensemble machine learning model.

--------------------------------------------------------------------------------
A. System Architecture
--------------------------------------------------------------------------------

Our system employs a unified research-production architecture, ensuring that
the exact same data, algorithms, and models are used for both experimental
validation and the live production chatbot.

**Core Components:**

1.  **DATA LAYER:** A unified dataset of 11,514 samples from four major
    sources, standardized into a schema including question, answer, expert
    score (1-5), and competency.

2.  **PREPROCESSING LAYER:** A standard NLTK pipeline for tokenization,
    lowercasing, stopword removal, and lemmatization, which feeds into both
    feature extraction and TF-IDF vectorization.

3.  **FEATURE EXTRACTION LAYER:** Computes an expanded set of 32 interpretable
    features across seven categories from the preprocessed text.

4.  **ML MODEL LAYER:** An ensemble model combining three individually trained
    classifiers: a hyperparameter-tuned Random Forest, a Gradient Boosting
    classifier, and a Support Vector Machine (SVM).

5.  **SCORING LAYER:** A weighted soft-voting mechanism in the ensemble model
    produces the final 1-5 score.

6.  **PRODUCTION INTERFACE:** A Flask-based conversational chatbot that uses
    the identical feature extraction and saved ensemble model, ensuring
    research findings directly translate to user-facing results.

**Technology Stack:**
- NLP: NLTK 3.8.1
- ML: scikit-learn 1.3.2 (RandomForestClassifier, GradientBoostingClassifier,
  SVC, VotingClassifier, TfidfVectorizer)
- Data: pandas 2.1.4, NumPy 1.26.2
- Language: Python 3.11.7

--------------------------------------------------------------------------------
B. Dataset Construction
--------------------------------------------------------------------------------

We constructed a large-scale, diverse dataset of 11,514 samples by unifying
four distinct sources, a significant expansion from our previous 3,334-sample
dataset.

**Data Sources:**

1.  **Behavioral Interview Dataset (1,470 samples):** Questions and answers
    focused on competencies like "Teamwork," "Leadership," and
    "Problem-Solving," with human-assigned scores.

2.  **Web Development Q&A (44 samples):** A smaller, specialized dataset for
    web development roles.

3.  **Stack Overflow Data (10,000 samples):** A large corpus of technical
    questions and user-submitted answers, with scores derived from community
    upvotes. This forms the backbone of our technical knowledge base.

4.  **Combined Training Data (additional samples):** A merged and cleaned
    dataset to ensure consistency.

**Data Cleaning and Unification:**

-   All datasets were merged, and columns were standardized to `question`,
    `answer`, and `human_score`.
-   Records with missing values or very short answers (<10 characters) were
    dropped.
-   Scores were normalized to an integer scale of 1 to 5.

**FINAL DATASET STATISTICS (11,514 Samples):**

Score Distribution:
- Score 1 (Poor): 2,380 samples (20.7%)
- Score 2 (Basic): 3,016 samples (26.2%)
- Score 3 (Adequate): 3,382 samples (29.4%)
- Score 4 (Good): 2,308 samples (20.0%)
- Score 5 (Excellent): 424 samples (3.7%)

This large, unified dataset provides the foundation for training a more
robust and generalizable model.

--------------------------------------------------------------------------------
C. Feature Engineering Framework
--------------------------------------------------------------------------------

Our framework was expanded to 32 features to capture more nuanced aspects of
answer quality.

**FEATURE CATEGORIES (32 Total):**

1.  **Basic Metrics (5 Features):** `word_count`, `sentence_count`,
    `avg_word_length`, `char_length`, `words_per_sentence`.

2.  **STAR Components (4 Features):** Binary flags for `has_situation`,
    `has_task`, `has_action`, `has_result` using keyword detection.

3.  **Professional Keywords (8 Features):** Counts of `action_verbs`,
    `technical_terms`, `metrics_mentions`, `professional_words`,
    `problem_solving`, `leadership_words`, `communication_words`, and
    `innovation_words`.

4.  **Structure & Quality (7 Features):** `has_numbers`, `question_marks`,
    `exclamation_marks`, `comma_count`, `uppercase_count`, `conjunctions`,
    and `is_complete`.

5.  **Confidence & Clarity (4 Features):** `has_examples`, `hedging_words`,
    `confident_words`, and `clarity_score`.

6.  **Advanced Metrics (4 Features):** `unique_word_ratio`, `complexity_score`,
    `technical_density`, and `professional_density`.

**Key New Features:**

-   **`complexity_score`:** A composite score combining average word length and
    words per sentence. This feature emerged as the #1 most important.
-   **`technical_density` & `professional_density`:** Ratios of specialized
    words to the total word count. These ranked #2 and #3 in importance,
    showing the model's shift towards evaluating language quality.

This expanded feature set allows the model to move beyond simple keyword
counting and assess the sophistication and density of the language used.

--------------------------------------------------------------------------------
D. NLTK Preprocessing Pipeline
--------------------------------------------------------------------------------

The five-stage NLTK pipeline remains consistent: Tokenization, Lowercasing,
Stopword Removal, Lemmatization, and Special Character Removal. This ensures
all text is normalized before being passed to the feature extractors and
TF-IDF vectorizer.

--------------------------------------------------------------------------------
E. Machine Learning Model
--------------------------------------------------------------------------------

To boost performance, we transitioned from a single Random Forest model to a
soft-voting ensemble of three classifiers.

**Ensemble Components:**

1.  **Random Forest (Tuned):** The core of the ensemble. Hyperparameters were
    tuned via `GridSearchCV`.
    -   `n_estimators`: 500
    -   `max_depth`: 25
    -   `min_samples_split`: 2
    -   `min_samples_leaf`: 1

2.  **Gradient Boosting Classifier:** A powerful boosting algorithm known for
    high accuracy.
    -   `n_estimators`: 300
    -   `max_depth`: 7
    -   `learning_rate`: 0.1

3.  **Support Vector Machine (SVM):** An effective classifier for high-dimensional
    spaces.
    -   `kernel`: 'rbf'
    -   `C`: 10
    -   `probability`: True (required for soft voting)

**Ensemble Configuration:**

-   **Algorithm:** `sklearn.ensemble.VotingClassifier`
-   **Voting:** `soft` (averages the probabilities from each model)
-   **Weights:** `[2, 2, 1]` (giving higher weight to the tree-based models,
    Random Forest and Gradient Boosting, which typically perform better on
    tabular data).

**Training Procedure:**

-   **Data Split:** 50/50 train-test split on the 11,514-sample dataset.
    (5,757 training, 5,757 testing).
-   **Stratification:** Preserves the 1-5 score distribution in both sets.
-   **Training:** The ensemble is trained on the 5,757 training samples.

This ensemble approach leverages the strengths of each individual model,
leading to higher accuracy and better generalization than any single model
could achieve alone.

================================================================================
END OF PART 3: METHODOLOGY
================================================================================
Total Word Count: ~1,850 words
Estimated Pages: 4-5 pages in IEEE two-column format
Status: COMPLETE - Focused on methodology only
Next: Create PART 4 (Experimental Results) when requested
================================================================================
