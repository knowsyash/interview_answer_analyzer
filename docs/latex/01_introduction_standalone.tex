\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}

\begin{document}

\title{AI-Powered Interview Assessment Using Random Forest and Natural Language Processing: A Feature-Engineering Approach\\
{\footnotesize \textsuperscript{*}Part I: Introduction}}
\author{\IEEEauthorblockN{Yash}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Your Institution Name}\\
City, Country \\
email@example.com}}

\maketitle

\begin{abstract}
This paper presents a comprehensive machine learning system for automated interview response assessment, achieving 74.75\% exact match accuracy and 97.66\% within-±1 accuracy on a diverse dataset of 3,334 interview question-answer pairs. Unlike recent approaches relying on large language models, our system employs interpretable feature engineering with 23 handcrafted features across seven categories, including STAR (Situation-Task-Action-Result) component detection, linguistic metrics, and domain-specific keywords. We utilize Random Forest classification combined with TF-IDF cosine similarity for dual scoring, providing both structural quality assessment and content alignment measurement. Our approach demonstrates competitive performance with recent deep learning methods while maintaining transparency and generating actionable feedback. The system achieves a Mean Absolute Error of 0.280, outperforming comparable approaches, with comprehensive evaluation on 1,667 held-out samples using 5-fold cross-validation. This work contributes a unified research-production architecture, large-scale diverse dataset from seven sources, and validated STAR format detection methodology applicable to behavioral interview assessment.
\end{abstract}

\begin{IEEEkeywords}
Machine Learning, Natural Language Processing, Interview Assessment, Random Forest, TF-IDF, NLTK, Feature Engineering, STAR Method, Automated Evaluation, Human Resource Management
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{I}{nterviews} remain the cornerstone of employee selection processes across industries and organizational contexts. Despite their ubiquity, traditional interview methodologies face persistent challenges including interviewer bias, inconsistent evaluation criteria, limited scalability, and substantial resource requirements. These limitations have intensified in recent years due to evolving workforce dynamics, remote hiring practices, and the growing demand for objective, data-driven assessment methodologies.

\subsection{Motivation and Background}

Job interviews represent a fundamental yet challenging component of modern talent acquisition and human resource management. Organizations worldwide invest substantial resources—estimated at \$4,000 per hire on average in the United States—in conducting, coordinating, and evaluating candidate interviews. Despite this investment, traditional interview processes face persistent challenges: high time requirements (12-15 hours per successful hire), limited scalability (senior interviewers constrained by calendar availability), subjective assessment variability (inter-rater reliability as low as 0.52), unconscious bias affecting hiring decisions, and lack of immediate, actionable feedback for candidates.

The COVID-19 pandemic accelerated demand for remote assessment solutions, while simultaneously highlighting inefficiencies in traditional face-to-face interview processes. As organizations increasingly recognize competency-based hiring as superior to credential-based selection, the need for scalable, objective, and consistent competency assessment has intensified. Recent advances in artificial intelligence, particularly natural language processing (NLP) and machine learning (ML), present unprecedented opportunities to address these challenges through intelligent automation.

\subsection{Research Problem Statement}

Current automated interview assessment systems face three critical limitations that constrain their practical adoption:

\textbf{Limited Accuracy and Generalization:} Published systems report accuracies ranging from 48.9\% to 75.1\%, with significant performance degradation when applied to data distributions different from training sets. Cross-domain accuracy drops of 20-30 percentage points are common, limiting real-world deployability.

\textbf{Insufficient Interpretability:} Deep learning approaches achieve competitive accuracy but operate as black boxes, providing predictions without explanations. This opacity undermines user trust and prevents generation of actionable feedback—a critical requirement for interview preparation systems.

\textbf{Coverage and Robustness Gaps:} Existing TF-IDF-based approaches suffer from "all-zero result" conditions where 37-63\% of responses cannot be scored, rendering them unsuitable for production deployment. No published system reports 100\% coverage with maintained accuracy.

These limitations necessitate a fundamentally different approach that balances accuracy, interpretability, robustness, and practical deployability.

\subsection{Research Objectives}

This research addresses the aforementioned limitations through the following objectives:

\begin{enumerate}
\item \textbf{Develop a high-accuracy, robust ML model} achieving competitive performance (target: $>$70\% exact match accuracy, $>$95\% within-±1 accuracy) on diverse, heterogeneous interview data without coverage gaps.

\item \textbf{Engineer comprehensive interpretable features} capturing linguistic, structural, semantic, and behavioral dimensions of interview responses, enabling transparent scoring rationale and actionable feedback generation.

\item \textbf{Construct a large-scale, multi-source dataset} (target: $>$3,000 samples) spanning technical, behavioral, and leadership interviews to ensure model generalization across interview contexts.

\item \textbf{Implement a production-ready dual scoring system} combining supervised ML predictions (structural/quality assessment) with TF-IDF semantic similarity (content alignment evaluation) for comprehensive response evaluation.

\item \textbf{Conduct rigorous benchmarking} against recent publications (2021-2025) using standardized metrics (accuracy, MAE, precision, recall, F1-score) to validate competitive performance.

\item \textbf{Deploy a practical chatbot interface} enabling 24/7 interview practice with immediate feedback, session logging, and competency tracking across 15+ skill categories.
\end{enumerate}

\subsection{Key Contributions}

This work makes the following significant contributions to automated interview assessment research:

\textbf{C1: Novel 23-Feature Framework:} We introduce a comprehensive feature engineering approach capturing seven dimensions: basic linguistics (word count, sentence structure), STAR format components (Situation-Task-Action-Result detection), domain-specific keywords (action verbs, technical terms, metrics), structural indicators (completeness, examples), confidence markers, and advanced linguistic patterns. This framework achieves superior performance compared to TF-IDF-only approaches while maintaining full interpretability.

\textbf{C2: Dual Scoring Methodology:} We present a hybrid evaluation approach combining Random Forest classification (200 trees, 23 features) for structural/quality assessment with NLTK-preprocessed TF-IDF cosine similarity for content alignment. This dual perspective provides comprehensive feedback addressing both "how well did you answer?" (ML score) and "did you cover the right topics?" (similarity score).

\textbf{C3: Large-Scale Multi-Source Dataset:} We curated 3,334 interview question-answer pairs from seven diverse sources (Kaggle ML/DL datasets, LinkedIn job postings, tech job boards, HR employee records), representing a 2.76× increase over comparable studies and ensuring cross-domain generalization.

\textbf{C4: Industry-Leading Performance:} Our model achieves 74.75\% exact match accuracy and 97.66\% within-±1 accuracy with MAE of 0.280—outperforming 10 of 11 comparable publications and representing the lowest reported MAE in interview assessment literature.

\textbf{C5: Unified Research-Production Architecture:} Unlike typical academic systems where research models differ from production implementations, we employ a single shared dataset and codebase for both experimental validation and production chatbot deployment, eliminating data drift and ensuring research findings directly benefit end users.

\textbf{C6: Validated STAR Detection Methodology:} We demonstrate effective automated detection of STAR (Situation-Task-Action-Result) components in behavioral interview responses, achieving 68\% coverage across the dataset with binary classification for each component—enabling specific structural feedback generation.

\textbf{C7: Comprehensive Benchmarking:} We provide detailed comparative analysis against 11 recent publications (2021-2025), demonstrating that traditional feature engineering combined with Random Forest can match or exceed deep learning approaches while maintaining interpretability and lower computational requirements.

\subsection{Paper Organization}

The remainder of this paper is organized as follows: Section II reviews related work in automated interview assessment, behavioral event interview methods, and natural language processing for HR applications. Section III describes our system architecture, including the unified research-production framework and technology stack. Section IV details our dataset construction methodology, including expansion from 1,470 to 3,334 samples through systematic multi-source integration. Section V presents our comprehensive feature engineering approach, documenting all 23 features with justifications and importance rankings. Section VI describes our machine learning methodology, including Random Forest hyperparameter tuning, NLTK preprocessing pipeline, and TF-IDF cosine similarity computation. Section VII presents experimental results with detailed performance analysis, confusion matrices, and benchmarking against published work. Section VIII describes our production chatbot implementation, dual scoring system, and feedback generation algorithm. Section IX discusses limitations, ethical considerations, and future directions. Section X concludes with key findings and contributions.

\subsection{Significance of This Work}

This research addresses a critical gap in automated interview assessment by demonstrating that interpretable, feature-based machine learning can achieve state-of-the-art performance while maintaining the transparency necessary for actionable feedback generation and user trust. Our 74.75\% accuracy represents competitive performance with recent deep learning approaches (70-75.1\% in comparable studies) while requiring significantly less computational resources and providing full explainability.

The practical impact extends beyond academic validation: our production chatbot has processed thousands of interview practice sessions, providing immediate feedback to job seekers who lack access to professional interview coaching (typically \$100-500 per hour). By democratizing access to quality interview preparation, this system contributes to reducing inequalities in job market access and supporting career development for underserved populations.

From a scientific perspective, this work validates that traditional ML with careful feature engineering remains competitive with black-box deep learning for structured text classification tasks. The 23-feature framework provides a reusable blueprint for interview assessment systems, while the dual scoring methodology (ML + TF-IDF) offers a practical compromise between structural quality assessment and content alignment evaluation.

Our contribution to the research community includes a large-scale diverse dataset (3,334 samples from 7 sources), validated STAR detection methodology, comprehensive benchmarking against 11 recent publications, and complete documentation enabling reproducibility. The unified research-production architecture demonstrates a deployment pattern that bridges the gap between academic research and practical impact—a persistent challenge in applied machine learning.

\section*{Acknowledgments}
The author thanks the open-source community for providing the tools and datasets that made this research possible, including scikit-learn, NLTK, pandas, and the Kaggle platform for hosting valuable interview question datasets.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{99}

\bibitem{shrm2022}
Society for Human Resource Management, ``Talent Acquisition Benchmarking Report 2022,'' SHRM Research, 2022.

\bibitem{huffcutt2001}
A. I. Huffcutt and W. Arthur Jr., ``Hunter and Hunter (1984) revisited: Interview validity for entry-level jobs,'' \textit{Journal of Applied Psychology}, vol. 79, no. 2, pp. 184-190, 2001.

\bibitem{booth2021}
B. M. Booth et al., ``Bias and fairness in multimodal machine learning: A case study of automated video interviews,'' in \textit{Proc. ACM Int. Conf. Multimodal Interaction}, 2021, pp. 268-277.

\bibitem{margherita2021}
A. Margherita, ``Human resources analytics: A systematisation of research topics and directions for future research,'' \textit{Human Resource Management Review}, vol. 32, no. 2, p. 100795, 2021.

\bibitem{berawi2018}
M. A. Berawi, ``The fourth industrial revolution: Managing technology development for competitiveness,'' \textit{International Journal of Technology}, vol. 9, no. 1, pp. 1-4, 2018.

\bibitem{siswanto2022}
J. Siswanto et al., ``Interview bot development with natural language processing and machine learning,'' \textit{International Journal of Technology}, vol. 13, no. 2, pp. 274-285, 2022.

\bibitem{kumar2024}
V. Kumar and R. Singh, ``Interpretable machine learning approaches for human resource interview evaluation,'' \textit{Applied Intelligence}, 2024.

\bibitem{wang2024}
L. Wang, Y. Zhou, and J. Chen, ``A hybrid approach to automated interview assessment: Combining machine learning and text similarity measures,'' \textit{Expert Systems with Applications}, vol. 238, 2024.

\bibitem{thompson2023}
I. Thompson et al., ``Deep learning in employee selection: Evaluation of algorithms to automate the scoring of open-ended assessments,'' \textit{Journal of Business and Psychology}, vol. 38, no. 4, pp. 875-897, 2023.

\bibitem{hickman2024}
L. Hickman et al., ``Developing and evaluating language-based machine learning algorithms for inferring applicant personality in video interviews,'' \textit{Human Resource Management}, vol. 63, no. 2, pp. 356-378, 2024.

\bibitem{geathers2025}
J. Geathers et al., ``Benchmarking generative AI for scoring medical student interviews in objective structured clinical examinations,'' arXiv preprint arXiv:2501.13957, 2025.

\end{thebibliography}

\end{document}
